<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>My portfolio</title>
		<hr class="major" />
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets1/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Welcome to my portfolio!</strong></a>
									<ul class="icons">
										
										<li><a href="https://www.linkedin.com/in/nhung-nguyen-hong/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://github.com/rosienhn" class="icon brands fa-github"><span class="label">Github</span></a></li>
										<li><a href="mailTo:nguyenhongnhunghr@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
									</ul>
								</header>

							
							
								<!-- Content -->
								<section>
									<header class="main">
										<h1>Building Cloud Data platform for Formula 1 data</h1>
										<h5>11 May, 2023</h5>
										<h3>Azure Data Factory | Databricks | Delta Lake | Azure ADLS Gen2 | Azure Key Vault</h3>
									</header>
									<span class="image main"><img src="" alt="" /></span>
									
									<h2 id="content">Project Overview</h2>
													
									<div>
									  <p>This project aims to build a data pipeline which is scheduled and triggered by Azure Data Factory for historical and up-to-date Formula1 data.	
										Databricks and Delta Lake were used to implement a solution using Lakehouse architecture.</p>
										<p>This is a good use case for Delta Lake, as it provides versioning and time travel capabilities to the data. By using Delta Lake, we can easily go back in time to see how the data looked at a specific point in time, and can also create new versions of the data as it changes over time. The approach is using PySpark in Databricks notebook to analyze the yearly race results and calculate the points for each team/driver based on their position. By doing this, we can determine the most dominant drivers/teams over specific time, which is valuable information for fans, sponsors, and teams.</p>
										<p>1. The initial data was obtained from the Ergast API, and subsequently tailored to suit the requirements of the project. Following this, the data was stored in DataLake Azure ADLS Gen2, where it was arranged according to the relevant event dates. The transformed data was then saved in both Delta Lake and Azure SQL Database.
											<br>2. Using databricks, data must be ingested into a Delta lake in delta formats, transformed into tables for reporting and analysis, and made available for machine learning/reporting and copy to Azure SQL database. 
											<br>3. Data pipelines must be scheduled, monitored, and alerts set up for pipeline failures.</p>
										<p>Detail codes can be found in my Github repository <a href="https://github.com/rosienhn/Formula1_DataPlatform_DataEngineer">here</a>. This project takes its inspiration from 2 Udemy courses: https://www.udemy.com/course/learn-azure-data-factory-from-scratch/ and https://www.udemy.com/course/azure-databricks-spark-core-for-data-engineers/.</p>
															
																					
														
																			
									<hr class="major" />

									<h2>Project Requirements</h2>

									<div class="scrollable-content">
										<div class="content">
											<ul>
												<ol>
													<li><b>Data Ingestion Requirements</b></li>
													<p>The task is to load historical F1 data into data lake Azure ADLS Gen2 in various formats and apply the appropriate schema to the data. The ingested data must have audit columns such as the ingested date and the source, and must be stored in Delta columnar format. The ingested data must be made available for various workloads, including machine learning, further transformation, BI reporting and analytical workloads via SQL. Additionally, the ingestion logic must be designed to handle incremental data loads.</p>
												
													<li><b>Data Transformation Requirements</b></li>
													<p>The requirements include combining required data items into transformed table for reporting/analysis purposes, adding audit columns to the transformed tables, and storing the transformed data in Delta format. The transformed data must be made available for various workloads, such as machine learning, BI reporting, and SQL analytics. Finally, the transformation logic must be designed to handle incremental loads.</p>
																							
													<li><b>Data Analytical Requirements</b></li>
													<p>The tasks include determining the most dominant drivers and teams over the last decade and all-time in Formula1, ranking them by dominance</p>
																																		
													<li><b>Data BI Reports Requirements</b></li>
														<ul>
															<li>Produce driver and constructor standings for current and past years.</li>
															<li>There is a predetermined set of data elements that should be used for the BI reporting. These data items could include specific metrics, dimensions, or other relevant information that is necessary for the analysis. The list of data items could be based on the organization's needs and requirements, industry standards, or other factors. By using a defined list of data items, the BI reporting can be standardized and consistent across different reports and analyses.</li>
														</ul>
																							
													<li><b>Scheduling Requirements</b></li>
														<ul>
															<li>Schedule pipelines to run at 11.30 p.m. every Sunday.</li>
															<li>Monitor pipeline status and re-run failed pipelines and set up alerts on failures.</li>
															<li>Set up alerts for pipeline failures.</li>
															<li>Enable time travel and ability to query data based on time.</li>
															<li>Roll back data to previous versions in case of issues.</li>
														</ul>
												</ol>
											</ul>
										</div>
									</div>
											
										
									<hr class="major" />
									<h2>Solution Architecture</h2>
										<ul>
											<li>The architecture includes three layers: the raw layer(Bronze layer), the processed layer(Silver layer), and the presentation layer(or Gold layer).</li>
											<li>Data will be uploaded to raw layer in csv/json format and ingested using Databricks Notebooks before being stored in the processed layer in delta format.</li>
											<li>The processed layer will include schema and partition information as well as additional audit information.</li>
											<li>Transformed data will be stored in the presentation layer in Delta format.</li>
											<li>Azure Data Factory will be used for orchestration, scheduling and monitoring the pipelines.</li>
																					  
										</ul>
										<p>The below image represents the solution architechture for this project:</p>
										<div class="image">
											<img src="image2/Solutionupdate.png" alt="" />
										</div>
										<p>The pipeline architecture is based on industry research and best practices, with reference to similar architecture from Microsoft and Databricks.</p>
										
										<ul class="actions">
											<li><a href="https://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/azure-databricks-modern-analytics-architecture" class="button big">To solution idea</a></li>
										</ul>
									
									<hr class="major" />

									<h2>Environment setup</h2>
									<ol type='i'>
										<li>Azure subcription
										<li>Azure Data Lake Storage Gen2
										<li>Azure Data Factory
										<li>Azure SQL Database
										<li>Azure Key Vault
										<li>Azure Databricks 
											<ul>
												<li> Create, configure and monitor Databricks clusters
												<li> Configure Databricks to use the ABFS driver to read and write data stored on Azure Data Lake Storage Gen2 using secrets stored in Azure Key Vault. A Pyspark function was used to mount the AZure ADLS Gen 2 to Databricks.
													<!-- Preformatted Code -->
													<h3></h3>
													<pre><code>def mount_adls(storage_account_name, container_name):
													
# Get secrets from Key Vault
client_id = dbutils.secrets.get(scope = 'formula1-scope', key = 'formula1-app-clientid')
tenant_id = dbutils.secrets.get(scope = 'formula1-scope', key = 'formula1-app-tenantid')
client_secret = dbutils.secrets.get(scope = 'formula1-scope', key = 'formula1-app-client-secret')
														
# Set spark configurations
configs = {"fs.azure.account.auth.type": "OAuth",
"fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
"fs.azure.account.oauth2.client.id": client_id,
"fs.azure.account.oauth2.client.secret": client_secret,
"fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}
														
# Unmount the mount point if it already exists
if any(mount.mountPoint == f"/mnt/{storage_account_name}/{container_name}" for mount in dbutils.fs.mounts()):
dbutils.fs.unmount(f"/mnt/{storage_account_name}/{container_name}")
														
# Mount the storage account container
dbutils.fs.mount(
source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
mount_point = f"/mnt/{storage_account_name}/{container_name}",
extra_configs = configs)
														
display(dbutils.fs.mounts())
</code></pre>
												<li> Use Delta Lake to implement a solution using Lakehouse architecture
											</ul>
										
										</ol>
									<ul> <p><b>Languages</b></p>
										<li>Spark SQL
										<li>PySpark</ul>
									<ul> <p><b>Technologies</b></p>
										<li>Data Pipeline
										<li>Spark (Spark streaming and Spark Machine Learning were not covered in this project)</ul>
								
								
								<hr class="major" />
									<h2>Projects in steps</h2>
									<p>To access Formula One data, the Ergast API was used to retrieve data for all races dating back to 1950. The API offers data in XML or JSON formats. CSV-formatted database tables can also be downloaded. Although the entire dataset can be obtained at once, I opted to process it incrementally to better understand real-world scenarios.</p>
									  <p>Formula One races usually take place on Sundays, but not every Sunday is reserved for a race. Races are typically scheduled for only 20 to 24 weeks per year, with no races scheduled during the remaining weeks. To implement incremental loading, I downloaded a zip file from the Ergast API and extracted its contents into CSV format. Each CSV file contained a single database table, which I split into smaller segments based on race IDs using Python.</p>
									  <p>For this project, I assumed that the first data containing data for all races up to race 1096 on the cutover date of December 24, 2022, will be received on that day, which is a Sunday. Data for the next two races, raceIDs 1098-1099, will be received on March 19, 2023 (data for race 1097 is not available on the Ergast API). On April 2, 2023, data for the latest race, race ID 1100, which is the "Australian Grand Prix" held on that day, will be received. After that, data will be updated every Sunday.</p>
									  
											<table>
												<p>Files and data used in the project are listed below:</p>
												<tr>
												  <th>File Name</th>
												  <th>Description</th>
												</tr>
												<tr>
												  <td>Circuits.csv, Drivers.csv, Constructors.csv, Races.csv</td>
												  <td>These files contain all the information related to the circuits, the drivers, the teams and the races. It was assumed that the information remains consistent throughout the race years.</td>
												</tr>
												<tr>
												  <td>Lap_times.csv, Pitstops.csv, Results.csv</td>
												  <td>The CSV files has been divided into three smaller files using Python. The first two contain race information from race_id 1 to 1096 and from 1098 to 1099, respectively. The third file includes information solely on the 1100th race.</td>
												</tr>
												<tr>
													<td>Qualifying.csv</td>
													<td>The CSV file was converted into three smaller json files using Python. The first two contain race information from race_id 1 to 1096 and from 1098 to 1099, respectively. The third file includes information solely on the 1100th race.</td>
												  </tr>
											  </table>
									 
									  <p>The Python script to split the files can be found <a href="https://github.com/rosienhn/Formula1_DataPlatform_DataEngineer/tree/main/Databricks%20Notebooks">here</a>.</p>

									  
									  <p>After the splitting process, the updated dataset was uploaded to DataLake Azure ADLS Gen2 and organized into three separate folders: 2022-12-24, 2023-03-19, and 2023-04-02.</p>
										
									  <p>It's recommended to review the Ergast Developer API website, which provides extensive information about the various tables and their relationships. The ERD diagram and Ergast Database User Guide can also be helpful resources.</p>
									 
									  <ul class="actions">
										<li><a href="http://ergast.com/images/ergast_db.png" class="button">Ergast Developer API</a></li>
									  </ul>
									  <strong><h3>STEP 1: Data Ingestion</h3></strong>
										<p>The data is ingested using the Spark DataFrame API, with partitioning and incremental loading based on file_date parameters. Circuits.csv, Drivers.csv, Constructors.csv, and Races.csv are fully loaded, as they are assumed to be static. Lap_times.csv, Pitstops.csv, Results.csv, and Qualifying.csv are loaded incrementally as they are updated weekly. A function is used to merge the incoming data with the existing Delta Lake data using a Python function. The function merges data stored in a Delta table with new data provided in the form of an input DataFrame. The merged data is written back to the same Delta table. If the Delta table does not already exist, the function writes the input DataFrame to the table using the Delta format and partitions it by the specified partition column. </p>
										<pre><code>def merge_delta_data(input_df,db_name,table_name,folder_path, merge_condition, partition_column):									
    spark.conf.set('spark.databricks.optimizer.dynamicPartitionPruning', 'true')

    from delta.tables import DeltaTable
    if (spark.catalog.tableExists(f'{db_name}.{table_name}')):
        deltaTable = DeltaTable.forPath(spark, f'{folder_path}/{table_name}')
        deltaTable.alias('r').merge(input_df.alias('f'), merge_condition) \
                            .whenMatchedUpdateAll() \
                            .whenNotMatchedInsertAll() \
                            .execute()
    else:
        input_df.write.mode('overwrite').partitionBy(partition_column).format('delta').saveAsTable(f'{db_name}.{table_name}')
											</code></pre>
										<p>The ingested data was written to processed layer in Delta format.</p>
										<strong><h3>STEP 2: Data Transformation</h3></strong>
										<p>To create a comprehensive race-results Delta table, the races, circuits, constructors, drivers, and results tables were joined together using Spark DataFrame API. This Delta table could also be loaded incrementally using Spark SQL. It contains the results of every race in every year, along with information about the race, circuit, driver, and their respective results, as well as the file_date and created_date. This table was then written to the presentation layer in Azure ADLS Gen2, enabling further analytical usage. Using this data, Top 10 Drivers/Teams of every race year and of all time were created and saved back into the Delta lake. </p>
										<p>Next, I created the drivers_standing and constructors_standing tables by processing the data from the race_results Delta table. The drivers_standing and constructors_standing tables provide valuable information such as the total points earned, ranking, and number of wins for drivers and constructors in each race year. These tables were also written to the presentation layer in Azure ADLS Gen2, which allows for further analysis.</p>
									  <h3>STEP 3: Copy transformed data to Azure SQL Database</h3>
									  <p>The transformed data was then copied to an Azure SQL database using the Pandas library. The necessary connection information, such as the hostname, database name, and port, were provided to establish the database connection.  Once the connection was established, the transformed data in the form of the DataFrame was written to Azure SQL database using the DataFrameWriter function with the specified parameters, including the overwrite mode and connection properties.</p>
									  <pre><code>
jdbcHostname = "....database.windows.net"
jdbcDatabase='db-....'
jdbcPort="1433"
properties= {"user":"user","password":"password"}
url = "jdbc:sqlserver://{0}:{1};database={2}".format(jdbcHostname,jdbcPort,jdbcDatabase)
df3=DataFrameWriter(df_top10_driver_sql)
df3.jdbc(url,table="[f1_reporting].[top10_drivers_every_year_sql]",mode="Overwrite",properties=properties)								
										
																				</code></pre>
									<p>All Databricks Notebooks can be found <a href="https://github.com/rosienhn/Formula1_DataPlatform_DataEngineer/tree/main/Databricks%20Notebooks">here</a>.</p>
									
									
									<hr class="major" />
									<h2>Data Orchestration in Azure Data Factory</h2>
									<div class="image">
										<p>I designed a master pipeline that consists of two sub-pipelines, each of which is responsible for a specific set of tasks. This architecture is well-suited for building a data processing pipeline that can handle large volumes of data and produce reliable, high-quality output. By breaking the pipeline down into smaller, more manageable tasks, I can ensure that each component is easier to build, test, and debug, which can help to improve overall pipeline efficiency and reliability.</p>
										<img src="image2/masterpipeline.png" alt="" />
										<p>The first sub-pipeline is responsible for ingesting data into the pipeline, while the second sub-pipeline is responsible for transforming that data into a format suitable for analysis and storage.</p>
										<img src="image2/Ingestion pipeline.png" alt="" />
										<p>Once the data has been transformed, data is stored in both Azure ADLS Gen2 in Delta format and Azure SQL database, making it available for use by downstream applications such as Power BI/BI Report for reporting and Machine Learning models.</p>
										<img src="image2/Transformation pipeline.png" alt="" />
										
									</div>
									</section>

								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<li><a href="my projects.html">My projects</a></li>
										<li><a href="https://github.com/rosienhn/my_resume/blob/main/NhungNguyen_Resume._May2023pdf.pdf">My Resume</a></li>
										
									</ul>
								</nav>

							<!-- Items -->
					<section class="wrapper style1 align-center">
						<div class="inner">
							<h3>Diploma & Certificates</h3>
							<!--<p>This is an <strong>Items</strong> element, and it's basically just a grid for organizing items of various types. You can customize its <span class="demo-controls">appearance with a number of modifiers</span>, as well as assign it an optional <code>onload</code> or <code>onscroll</code> transition modifier (<a href="#reference-items">details</a>).</p>-->
							<div class="items style1 medium onscroll-fade-in">
								<section>
									<span class="icon style2 major fa-gem"></span>
									<h3>Microsoft Azure Data Engineer</h3>
									<p>DP-203 certification-to be obtained</p>
								</section>
								<section>
									<span class="icon solid style2 major fa-save"></span>
									<h3>Certified Microsoft Azure Data Analyst</h3>
									<p>PL-300 certification</p>
								</section>
								<section>
									<span class="icon solid style2 major fa-chart-bar"></span>
									<h3>Microsoft Azure Data Fundamental</h3>
									<p>AZ-900 Certification</p>
								</section>
								<section>
									<span class="icon style2 major fa-paper-plane"></span>
									<h3>Msc Human Resource Studies</h3>
									<p>Tilburg University</p>
								</section>
							</div>

						<!-- Section -->
							<section>
								<header class="major">
									<h2>Get in touch</h2>
								</header>
								<p>Like what you see?

									I'd love to hear from you!</p>
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="#">nguyenhongnhunghr@gmail.com</a></li>
									<li class="icon solid fa-phone"> 0628501248</li>
								</ul>
							</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Copyright 2023. All rights reserved<a href="#"></a><a href="#"></a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets1/js/jquery.min.js"></script>
			<script src="assets1/js/browser.min.js"></script>
			<script src="assets1/js/breakpoints.min.js"></script>
			<script src="assets1/js/util.js"></script>
			<script src="assets1/js/main.js"></script>

	</body>
</html>